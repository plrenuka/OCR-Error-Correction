{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lYezYnh2Gd_k"
   },
   "source": [
    "# Natural Language Processing â€“\n",
    "# Project 2: **OCR Error Correction using Character Based Language Modeling**\n",
    "\n",
    "Student Name - Renuka Lalit Patil\n",
    "\n",
    "Student no. - R00195785"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "obacp106EjTv"
   },
   "source": [
    "## Installation of required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "enCvtPa7Y4EY",
    "outputId": "00bc545f-ff9b-448d-c358-bc3fbea254bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.8.2+zzzcolab20220719082949)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (4.1.1)\n",
      "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.26.0)\n",
      "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.2.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.17.3)\n",
      "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.47.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow) (57.4.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n",
      "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.5.3)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.21.6)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (14.0.6)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow) (1.5.2)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.0.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.23.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.35.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.9)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.2.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (4.12.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (3.8.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2022.6.15)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (1.24.3)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (3.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JhsLALS0D92V"
   },
   "source": [
    "# **Part 2** : Implementation of a character based Language Model for word suggestion\n",
    "\n",
    "please refer run.py for part 1 - Decontraction and tokenization of OCR output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "90hAgkkO57wH"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n9Jca_ImXsfS",
    "outputId": "53a5387f-c9d0-4191-96cb-f8f52d07a033"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package europarl_raw to /root/nltk_data...\n",
      "[nltk_data]   Package europarl_raw is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using Europarl english corpus - http://www.statmt.org/europarl/\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus.europarl_raw import english\n",
    "nltk.download('europarl_raw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RfsQpswjGcD_"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8NMi4JmJ6Atj",
    "outputId": "bb06ccd7-9c3b-42e4-b989-e93c233a06b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3059995\n"
     ]
    }
   ],
   "source": [
    "text = english.raw()\n",
    "text = text + 'Unitek'\n",
    "text = text.lower()\n",
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "id": "vtIvLg5WRtWf"
   },
   "outputs": [],
   "source": [
    "# Tokenization of the string (word of phrase)\n",
    "\n",
    "def Tokenize(text):\n",
    "  tokens = list(nltk.word_tokenize(text))\n",
    "  # print(tokens)\n",
    "  return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "HNuj3cjFg9Eb"
   },
   "outputs": [],
   "source": [
    "# pre processing the text data\n",
    "def clean_text(text):\n",
    "  text = text.replace('--', ' ')\n",
    "  text = text.replace('\\n', ' ')\n",
    "  tokens = Tokenize(text)\n",
    "  tokens = [word for word in tokens if word.isalpha()]\n",
    "  text = ' '.join(tokens)\n",
    "  return text, tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mNqNj89kg95I",
    "outputId": "9c50465e-c409-4e58-a780-e0aaf8e6117e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "494840\n"
     ]
    }
   ],
   "source": [
    "text, tokens = clean_text(text)\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DI2yF_vhy6dq"
   },
   "source": [
    "## **LSTM model training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EyPZkQd5c2Ul",
    "outputId": "514a8e7c-d148-4331-f4d1-ced5e20061dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of characters:49\n"
     ]
    }
   ],
   "source": [
    "# calculating number of characters \n",
    "chars = sorted(list(set(text))) \n",
    "char_indx = dict((char, chars.index(char)) for char in chars)     # dictionary used for mapping\n",
    "vocab_size = len(chars)\n",
    "print(f'number of characters:{len(chars)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UgclQq-OaM9H",
    "outputId": "3febf2c2-fe4f-41e7-ee25-2f2734d794d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences: 2901656\n"
     ]
    }
   ],
   "source": [
    "max_len = 15\n",
    "sequences = []\n",
    "next_chars = []\n",
    "# segmentation and encoding data for training\n",
    "for i in range(0, len(text) - max_len, 1):\n",
    "    t = text[i: i + max_len]\n",
    "    encoded = [char_indx[char] for char in t]\n",
    "    sequences.append(encoded)                   # feature seq\n",
    "             \n",
    "    next_chars.append(char_indx[text[i + max_len]])              # targets\n",
    "    \n",
    "print(f'Number of sequences: {len(sequences)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZNVXKxU1L0De"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oajc8f4xdCsj",
    "outputId": "01f46357-0607-4864-9496-f6add412a079"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  \n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# one hot encoding\n",
    "x = np.zeros((len(sequences), max_len, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sequences), len(chars)), dtype=np.bool)\n",
    "for i, sent in enumerate(sequences):\n",
    "    for t, char in enumerate(sent):\n",
    "        x[i, t, char] = 1 \n",
    "    y[i, next_chars[i]] = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uhBgLtl9dGaH",
    "outputId": "a68cd51b-8a3b-479a-a43a-43cb09c27a8b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 64)                29184     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 49)                3185      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 32,369\n",
      "Trainable params: 32,369\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.LSTM(64, input_shape=(max_len, vocab_size)))\n",
    "model.add(tf.keras.layers.Dense(len(chars), activation='softmax'))\n",
    "print(model.summary())\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xSmZ6giHdVb1"
   },
   "outputs": [],
   "source": [
    "# train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "chJtOk6PdaSv",
    "outputId": "7e912864-a6ec-40af-b6f8-f2088551b38a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "45339/45339 [==============================] - 496s 11ms/step - loss: 1.6508 - accuracy: 0.5075\n",
      "Epoch 2/6\n",
      "45339/45339 [==============================] - 502s 11ms/step - loss: 1.3438 - accuracy: 0.5945\n",
      "Epoch 3/6\n",
      "45339/45339 [==============================] - 498s 11ms/step - loss: 1.2753 - accuracy: 0.6133\n",
      "Epoch 4/6\n",
      "45339/45339 [==============================] - 484s 11ms/step - loss: 1.2410 - accuracy: 0.6224\n",
      "Epoch 5/6\n",
      "45339/45339 [==============================] - 488s 11ms/step - loss: 1.2195 - accuracy: 0.6279\n",
      "Epoch 6/6\n",
      "45339/45339 [==============================] - 485s 11ms/step - loss: 1.2045 - accuracy: 0.6318\n"
     ]
    }
   ],
   "source": [
    "\n",
    "history = model.fit(x, y, batch_size=64, epochs = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NQOWRMJadfA9"
   },
   "outputs": [],
   "source": [
    "# saving the model and character mapping dictionary\n",
    "from pickle import dump\n",
    "model.save('/content/ocr_text-gen-lstm.h5')\n",
    "dump(char_indx, open('/content/mapping.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N-8-jcaiDYjO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OMxgr6_VzHCM"
   },
   "source": [
    "# 3.1) Check if the proposed word is an English word, if not, suggest possible words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "A-ja94aMDMRx"
   },
   "outputs": [],
   "source": [
    "# https://machinelearningmastery.com/develop-character-based-neural-language-model-keras/\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "from pickle import load\n",
    " \n",
    "# generate a sequence of characters with a language model\n",
    "def generate_seq(model, mapping, reversed_map, seq_length, seed_text, n_chars):\n",
    "  \n",
    "  # generate a fixed number of characters\n",
    "  encoded = [mapping[char] for char in seed_text]\n",
    "  # truncate sequences to a fixed length\n",
    "  encoded = tf.keras.preprocessing.sequence.pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "  # one hot encode\n",
    "  encoded = tf.keras.utils.to_categorical(encoded, num_classes=len(mapping))\n",
    "  # predict character\n",
    "  predict_x = model.predict(encoded, verbose=0)\n",
    "  # selecting best 10 charchters \n",
    "  # best_fit = np.argmax(predict_x,axis=1)\n",
    "  best_fit = predict_x[0].argsort()[::-1][:10]\n",
    "  sugg_words = []\n",
    "  in_text_list = dict()\n",
    "  for a in best_fit:\n",
    "    if reversed_map[a] == \" \":\n",
    "      in_text_list[a] = seed_text\n",
    "    else:\n",
    "      in_text_list[a] = seed_text + reversed_map[a]\n",
    "\n",
    "  # iterating further for each selected character\n",
    "  for i in best_fit:\n",
    "    in_text = in_text_list[i]\n",
    "    for _ in range(n_chars-1):\n",
    "      encoded = [mapping[char] for char in in_text]\n",
    "      # truncate sequences to a fixed length\n",
    "      encoded = tf.keras.preprocessing.sequence.pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "      # one hot encode\n",
    "      encoded = tf.keras.utils.to_categorical(encoded, num_classes=len(mapping))\n",
    "      # predict character\n",
    "      predict_x = model.predict(encoded, verbose=0)\n",
    "      best_fit_ = np.argmax(predict_x,axis=1)\n",
    "      if reversed_map[best_fit_[0]] == \" \":\n",
    "        break\n",
    "      in_text_list[i] = in_text_list[i] + reversed_map[best_fit_[0]]\n",
    "      in_text = in_text + reversed_map[best_fit_[0]]\n",
    "      \n",
    "    sugg_words.append(in_text_list[i])\n",
    "  # print(sugg_words)\n",
    "  return sugg_words\n",
    "  \n",
    "# ----------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zHWUxNVT0t22",
    "outputId": "7a12793f-98b4-4d41-8b39-aa1b2bd14f8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of all OCR words - ['unetek']\n",
      "List of non english words - ['unetek']\n",
      "suggested english words from corpus - [['us', 'up', 'ur', 'untelly', 'udgenin', 'uch', 'umpooct', 'ually', 'uedes', 'utalial', 'untelly', 'under', 'unally', 'unoll', 'union', 'untelly', 'unfoutt', 'unglican', 'unemple', 'unryies', 'unemple', 'unesport', 'unemple', 'unea', 'unelardic', 'uned', 'unepar', 'unequalit', 'unera', 'uneelly', 'unetrested', 'unetion', 'unetreste', 'uneterial', 'uneth', 'unettrest', 'uneta', 'unetgor', 'unetper', 'unetly', 'uneterial', 'uneterial', 'unetely', 'uneted', 'unetect', 'unetes', 'uneteer', 'unetemer', 'uneteg', 'unetear']]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# load the model\n",
    "model = tf.keras.models.load_model('/content/ocr_text-gen-lstm (2).h5')\n",
    "# load the mapping\n",
    "mapping = load(open('/content/mapping (2).pkl', 'rb'))\n",
    "\n",
    "# reverse the mapping \n",
    "reversed_map = dict()\n",
    "key_list = list(mapping.keys())\n",
    "val_list = list(mapping.values())\n",
    "n = len(key_list)\n",
    "for i in range(n):\n",
    "  key = val_list[i]\n",
    "  val = key_list[i]\n",
    "  reversed_map[key] = val\n",
    "# print(f' The reversed mapping dict- {reversedDict}')\n",
    "\n",
    "# Read text detected by OCR model\n",
    "text_ocr = open(\"OCR_output.txt\", \"r\")\n",
    "x = text_ocr.read()\n",
    "words = x.split()\n",
    "words = [w.lower() for w in words]\n",
    "print(f'List of all OCR words - {words}')\n",
    "\n",
    "# Check if the word is an english word\n",
    "not_eng = []\n",
    "not_eng_indx = []\n",
    "for w in range(len(words)):\n",
    "  if words[w] not in tokens:\n",
    "    not_eng.append(words[w])\n",
    "    not_eng_indx.append(w)\n",
    "print(f'List of non english words - {not_eng}')\n",
    "\n",
    "# Suggest english words using loaded model\n",
    "pred_list_all = []\n",
    "for word in not_eng:\n",
    "  pred_list = []\n",
    "  w = ''\n",
    "  # adding characters one by one into the model input \n",
    "  for c in range(len(word)-1):\n",
    "    w = w + word[c]\n",
    "    # print(w, len(word))\n",
    "    pred_list.append(generate_seq(model, mapping, reversed_map, 15, w , len(word)))\n",
    "  pred_list_all.append([x for P in pred_list for x in P])\n",
    "\n",
    "print(f'suggested english words from corpus - {pred_list_all}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JzmrkG_pivl9"
   },
   "source": [
    "# 3.2) Choose the word with the shortest weighted edit distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "olgXmfjyIaES",
    "outputId": "7403a1ac-e4ad-40b1-f20c-b06f1798a0f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: weighted-levenshtein in /usr/local/lib/python3.7/dist-packages (0.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install weighted-levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JizwHh3CIb4m",
    "outputId": "7aa29f4f-04fa-49aa-b35a-61008b536db4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best fitting suggestions - ['uneted']\n",
      "final text after replacing non english words - uneted\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/infoscout/weighted-levenshtein\n",
    "\n",
    "\n",
    "from weighted_levenshtein import lev\n",
    "actual_word = 'Unitek'\n",
    "actual_word = actual_word.lower()\n",
    "\n",
    "final_out_list = []\n",
    "\n",
    "for e in range(len(pred_list_all)):\n",
    "  cost_list = []\n",
    "  for pred in pred_list_all[e]:\n",
    "    x = lev(actual_word, pred)        \n",
    "    cost_list.append(x)\n",
    "  # selecting minimum distance suggested word \n",
    "  min_dist_idx = np.argmin(cost_list)\n",
    "  final_out_list.append(pred_list_all[e][min_dist_idx])\n",
    "\n",
    "print(f'Best fitting suggestions - {final_out_list}')\n",
    "\n",
    "\n",
    "# replacing non english words in OCR text\n",
    "for i in range(len(final_out_list)):\n",
    "  words[not_eng_indx[i]] = final_out_list[i]\n",
    "\n",
    "final_text = ' '.join(words)\n",
    "print(f'final text after replacing non english words - {final_text}')\n",
    "# writing replaced output text in text file for contraction\n",
    "with open(\"Sugg_output.txt\", \"w\") as text_file:\n",
    "    text_file.write(str(final_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8oCvaFexHQMu"
   },
   "source": [
    "please refer contract.py for part 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6hxnY82qzcMS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ub9c70beiQV6"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Patil_Renuka_NLP_2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
